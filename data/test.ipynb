{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from japan_ha.constant.training_pipeline import TARGET_COLUMN\n",
    "from japan_ha.constant.training_pipeline import DATA_TRANSFORMATION_IMPUTER_PARAMS\n",
    "\n",
    "from japan_ha.entity.artifacts_entity import (\n",
    "   DataTransformationArtifact, DataValidationArtifact\n",
    ")\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's Pipeline for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from japan_ha.entity.config_entity import DataTransformationConfig\n",
    "from japan_ha.exception.exception import JapanHeartAttackException\n",
    "from japan_ha.logging.logger import logging\n",
    "from japan_ha.utils.main_utils.utils import save_numpy_array_data,save_object\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's Pipeline for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"japan_heart_attack_dataset.csv\")\n",
    "\n",
    "def get_data_transformer_object():\n",
    "        '''\n",
    "        This function is responsible for data transformation.\n",
    "        '''\n",
    "        try:\n",
    "            numerical_columns =  ['Age', 'Cholesterol_Level','Stress_Levels', 'BMI','Heart_Rate','Systolic_BP','Diastolic_BP']\n",
    "            categorical_columns =  ['Gender', 'Region', 'Smoking_History', 'Diabetes_History',\n",
    "                                    'Hypertension_History', 'Diet_Quality', 'Alcohol_Consumption', 'Family_History'\n",
    "                                    ,\"Physical_Activity\"]\n",
    "\n",
    "            # Numerical pipeline with KNNImputer and StandardScaler\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", KNNImputer()),  # Use KNNImputer instead of SimpleImputer\n",
    "                    (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Categorical pipeline with KNNImputer, OneHotEncoder, and StandardScaler\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Use KNNImputer instead of SimpleImputer\n",
    "                    (\"one_hot_encoder\", OneHotEncoder()),\n",
    "                    (\"scaler\", StandardScaler(with_mean=False))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical Columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical Columns: {numerical_columns}\")\n",
    "\n",
    "            # Combine numerical and categorical pipelines using ColumnTransformer\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "                    (\"cat_pipeline\", cat_pipeline, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Add SMOTE to the pipeline using imblearn's Pipeline\n",
    "            processor = ImbPipeline(\n",
    "                steps=[\n",
    "                    (\"preprocessor\", preprocessor),  # Apply preprocessing\n",
    "                    (\"smote\", SMOTE(sampling_strategy=\"minority\"))  # Apply SMOTE\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            return processor\n",
    "\n",
    "        except Exception as e:\n",
    "            raise JapanHeartAttackException(e, sys)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df,test_df=train_test_split(df,test_size=0.2)\n",
    "\n",
    "\n",
    "input_feature_train_df=train_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "target_feature_train_df=train_df[TARGET_COLUMN]\n",
    "\n",
    "input_feature_test_df=test_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "target_feature_test_df=test_df[TARGET_COLUMN]\n",
    "\n",
    "preprocessor=get_data_transformer_object()\n",
    "# Fit and resample the preprocessor on the training data\n",
    "transformed_input_train_feature, target_feature_train_df = preprocessor.fit_resample(input_feature_train_df, target_feature_train_df)\n",
    "\n",
    "# Fit and resample the preprocessor on the test data\n",
    "transformed_input_test_feature, target_feature_test_df = preprocessor.fit_resample(input_feature_test_df, target_feature_test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
